{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE   = int(1e5)\n",
    "BATCH_SIZE    = 64\n",
    "GAMMA         = 0.99 # discount factor\n",
    "TAU           = 1e-3 # soft update of target parameter\n",
    "LEARNING_RATE = 5e-4\n",
    "UPDATE_EVERY  = 4    # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" Agent Policy Network Model \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state:torch.Tensor):\n",
    "        \"\"\" state -> action values \"\"\"\n",
    "        print(state)\n",
    "        x = torch.flatten(state)\n",
    "        print(state)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.]])\n",
      "torch.Size([2, 16])\n",
      "tensor([[12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.]])\n",
      "tensor([[12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
      "         12., 12.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 16x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[424], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(state\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 5\u001b[0m \u001b[39mprint\u001b[39m(q(state))\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[422], line 14\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     12\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(state)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m---> 14\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 16x64)"
     ]
    }
   ],
   "source": [
    "q = QNetwork(16,4)\n",
    "state = torch.from_numpy(np.array([12]*16*2).reshape(2,16)).float()\n",
    "print(state)\n",
    "print(state.shape)\n",
    "print(q(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]],\n",
      "\n",
      "        [[12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]],\n",
      "\n",
      "        [[12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]],\n",
      "\n",
      "        [[12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.],\n",
      "         [12., 12., 12.,  ..., 12., 12., 12.]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1024 and 16x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[412], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[0;32m      3\u001b[0m q\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(q(batch))\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[398], line 12\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m\"\"\" state -> action values \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(state)\n\u001b[1;32m---> 12\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     13\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     14\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1024 and 16x64)"
     ]
    }
   ],
   "source": [
    "batch = torch.from_numpy(np.array([12]*16*BATCH_SIZE).reshape(4,4,BATCH_SIZE)).float().unsqueeze(0)\n",
    "print(batch)\n",
    "print(q(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Fixed size buffer to store experience tuples \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('experience', field_names=[\n",
    "                                     'state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.memory: deque[self.experience] = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"  \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([e.state for e in experiences if e is not None])).to(device).float()\n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e.action for e in experiences if e is not None])).to(device).float()\n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e.reward for e in experiences if e is not None])).to(device).float()\n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([e.next_state for e in experiences if e is not None])).to(device).float()\n",
    "        dones = torch.from_numpy(np.vstack(\n",
    "            [e.done for e in experiences if e is not None]).astype(np.uint8)).to(device).float()\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=ReplayBuffer(4, BUFFER_SIZE, BATCH_SIZE)\n",
    "for i in range(100):\n",
    "    b.add(np.array([123]*16).reshape(4,4),0,3,np.array([123]*16).reshape(4,4),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         ...,\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.]], device='cuda:0'),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], device='cuda:0'),\n",
       " tensor([[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]], device='cuda:0'),\n",
       " tensor([[123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         ...,\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.],\n",
       "         [123., 123., 123., 123.]], device='cuda:0'),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "        # replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "        self.time_step = 0\n",
    "        self.eps = 0.0\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.time_step = (self.time_step+1) % UPDATE_EVERY\n",
    "        if self.time_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience)\n",
    "\n",
    "    def act(self, state, eps=0.0, train=True):\n",
    "        state = torch.from_numpy(state).to(device).float().unsqueeze(0)\n",
    "        print('state',state)\n",
    "        print(torch.flatten(state))\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state).to(device)\n",
    "        self.qnetwork_local.train()\n",
    "        print(action_values)\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy()) #  \"addmm_cuda\" not implemented for 'Long'\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experience):\n",
    "        \"\"\" Update parameters using batch of experience tuples \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experience\n",
    "        q_targets_next = self.qnetwork_target(\n",
    "            next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards+self.gamma*q_targets_next*(1-dones)\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        # Compute the loss and gradient\n",
    "        loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def state_to_features(self, state: np.ndarray):\n",
    "        return torch.from_numpy(state.flatten()).to(device).float().unsqueeze(0)\n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\" θ_target = τ*θ_local + (1 - τ)*θ_target \n",
    "        copy the weights of the local model to the target model\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau*local_model.data+(1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Agent(16, 4, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.0005\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.step(np.array([123]*16),0,3,np.array([123]*16),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state tensor([[[1.8000, 1.8000, 1.8000, 1.8000],\n",
      "         [1.8000, 1.8000, 1.8000, 1.8000],\n",
      "         [1.8000, 1.8000, 1.8000, 1.8000],\n",
      "         [1.8000, 1.8000, 1.8000, 1.8000]]], device='cuda:0')\n",
      "tensor([1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000,\n",
      "        1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000, 1.8000],\n",
      "       device='cuda:0')\n",
      "tensor([-0.1142, -0.0114, -0.0334,  0.1490], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.act(np.array([1.8]*16).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1024 and 16x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[408], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[39m.\u001b[39;49mlearn(b\u001b[39m.\u001b[39;49msample())\n",
      "Cell \u001b[1;32mIn[403], line 42\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m\"\"\" Update parameters using batch of experience tuples \"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m experience\n\u001b[1;32m---> 42\u001b[0m q_targets_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnetwork_target(\n\u001b[0;32m     43\u001b[0m     next_states)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m q_targets \u001b[39m=\u001b[39m rewards\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\u001b[39m*\u001b[39mq_targets_next\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mdones)\n\u001b[0;32m     45\u001b[0m q_expected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local(states)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, actions)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[398], line 12\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m\"\"\" state -> action values \"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(state)\n\u001b[1;32m---> 12\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     13\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     14\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\machine-learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1024 and 16x64)"
     ]
    }
   ],
   "source": [
    "a.learn(b.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps-eps_decay)\n",
    "        if episode % 100 == 0:\n",
    "            print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3830843451.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[391], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=8, action_size=4, learning_rate=LEARNING_RATE)\n",
    "scores = dqn(agent, n_episodes=2000, max_time_step=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d155ddd594e93c13dc604d9fde6a83ce6733a6c3fc0471778cef1ca6f99b89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
