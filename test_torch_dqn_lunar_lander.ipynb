{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\felixwong\\Desktop\\py alg practice\\machine_learning\\test_torch.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/machine_learning/test_torch.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mLunarLander-v2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/machine_learning/test_torch.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/machine_learning/test_torch.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mState shape: \u001b[39m\u001b[39m'\u001b[39m, env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:619\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     env_creator \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mentry_point\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m     env_creator \u001b[39m=\u001b[39m load(spec_\u001b[39m.\u001b[39;49mentry_point)\n\u001b[0;32m    621\u001b[0m mode \u001b[39m=\u001b[39m _kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    622\u001b[0m apply_human_rendering \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:62\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m    Calls the environment constructor\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[0;32m     63\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbipedal_walker\u001b[39;00m \u001b[39mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcar_racing\u001b[39;00m \u001b[39mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlunar_lander\u001b[39;00m \u001b[39mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\u001b[39m\"\u001b[39m\u001b[39mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     29\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE   = int(1e5)\n",
    "BATCH_SIZE    = 64\n",
    "GAMMA         = 0.99 # discount factor\n",
    "TAU           = 1e-3 # soft update of target parameter\n",
    "LEARNING_RATE = 5e-4\n",
    "UPDATE_EVERY  = 4    # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" Agent Policy Network Model \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\" state -> action values \"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.qnetwork_local.parameters(), lr=learning_rate)\n",
    "        # replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "        self.time_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.time_step(self.time_step+1) % UPDATE_EVERY\n",
    "        if self.time_step == 0:\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.0):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experience, gamma):\n",
    "        \"\"\" Update parameters using batch of experience tuples \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experience\n",
    "        q_targets_next = self.qnetwork_target(\n",
    "            next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards+gamma*q_targets_next*(1-dones)\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        # Compute the loss and gradient\n",
    "        loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\" θ_target = τ*θ_local + (1 - τ)*θ_target \n",
    "        copy the weights of the local model to the target model\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau*local_model.data+(1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Fixed size buffer to store experience tuples \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple('experience', field_names=[\n",
    "                                     'state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.memory: deque[self.experience] = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"  \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e.action for e in experiences if e is not None])).float()\n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack(\n",
    "            [e.done for e in experiences if e is not None])).astype(np.uint8).float()\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps-eps_decay)\n",
    "        if episode % 100 == 0:\n",
    "            print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=8, action_size=4, learning_rate=LEARNING_RATE)\n",
    "scores = dqn(agent, n_episodes=2000, max_time_step=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
