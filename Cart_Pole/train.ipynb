{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "from collections import deque\n",
    "from typing import Optional, Iterable\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(deque):\n",
    "    def __init__(self, iterable: Optional[Iterable]=None, maxlen: Optional[int]=None):\n",
    "        if iterable is None:\n",
    "            super(Recorder, self).__init__(maxlen=maxlen)\n",
    "        else:\n",
    "            super(Recorder, self).__init__(iterable, maxlen=maxlen)\n",
    "\n",
    "    def tensor(self):\n",
    "        \"\"\" Give [B, C, H, W] tensor for images \n",
    "        e.g. [H, W]x4 -> [4, H, W]\n",
    "        \"\"\"\n",
    "        return torch.stack(tuple(torch.tensor(x, device=device) for x in self)).to(device)\n",
    "    \n",
    "    def numpy(self)->np.ndarray:\n",
    "        return self.tensor().cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIDTH  = 240\n",
    "HEIGHT = 160\n",
    "def preprocess_image(image: np.ndarray, width=WIDTH, height=HEIGHT):\n",
    "    _, gray = cv.threshold(cv.cvtColor(image, cv.COLOR_BGR2GRAY), 200, 255,cv.THRESH_BINARY)\n",
    "    # gray:np.ndarray = gray[(gray.shape[0]-WIDTH)//2:(gray.shape[0]-WIDTH)//2+WIDTH,(gray.shape[1]-HEIGHT)//2:(gray.shape[1]-HEIGHT)//2+HEIGHT]\n",
    "    gray = cv.resize(gray, (width, height))\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "recorder = Recorder(maxlen=4)\n",
    "for _ in range(4):\n",
    "    env.step(random.choice([0,1]))\n",
    "    image : np.ndarray = env.render()\n",
    "    recorder.append(preprocess_image(image))\n",
    "i:np.ndarray = env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 240)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_image(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "replay = ReplayBuffer(buffer_size=100, batch_size=32)\n",
    "for _ in range(100):\n",
    "    replay.add(recorder.numpy(), 0, 0, recorder.numpy(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 160, 240])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recorder.tensor().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 160, 240])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay.sample().state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012],\n",
       "        [-0.1865,  0.0012]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Model import QNetwork\n",
    "model = QNetwork(action_size=2).to(device)\n",
    "model(replay.sample().state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE         = 100\n",
    "BATCH_SIZE          = 32\n",
    "GAMMA               = 0.99  # discount factor\n",
    "TAU                 = 0.1   # soft update of target parameter\n",
    "LEARNING_RATE       = 1e-2\n",
    "UPDATE_EVERY        = 2    # how often to update the local\n",
    "TARGET_UPDATE_EVERY = 8    # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import Agent\n",
    "agent = Agent('not_used', 2, LEARNING_RATE, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(5, 5), stride=(3, 3))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=52992, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(replay.memory[0].state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627],\n",
       "        [ 0.2144, -1.3627]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local(replay.sample().state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0460, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(replay.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.step(*replay.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-279739.9062,  -24406.8633]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_value(replay.memory[0].state, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelissi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Desktop\\py-practice\\machine_learning\\codebase---machine-learning\\Cart_Pole\\wandb\\run-20230222_235634-ox773zoc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felissi/cart-pole/runs/ox773zoc' target=\"_blank\">revived-forest-9</a></strong> to <a href='https://wandb.ai/felissi/cart-pole' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felissi/cart-pole' target=\"_blank\">https://wandb.ai/felissi/cart-pole</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felissi/cart-pole/runs/ox773zoc' target=\"_blank\">https://wandb.ai/felissi/cart-pole/runs/ox773zoc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/felissi/cart-pole/runs/ox773zoc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1369e7c01c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.wandb.yaml', 'r') as f:\n",
    "    key = yaml.safe_load(f.read())['key']\n",
    "wandb.login(key=key)\n",
    "wandb.init(project=\"cart-pole\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(agent.qnetwork_local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    num_rounds = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    env = gym.make('CartPole-v1',render_mode='rgb_array')\n",
    "    recorder = Recorder(maxlen=4)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        env.reset()\n",
    "        # Get first 4 frames\n",
    "        for _ in range(4):\n",
    "            env.step(random.choice([0,1]))\n",
    "            image : np.ndarray = env.render()\n",
    "            recorder.append(preprocess_image(image))\n",
    "            cv.waitKey(25)\n",
    "            cv.imshow('',image)\n",
    "        state = recorder.numpy()\n",
    "        accumulate_reward = 0\n",
    "        rounds = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            cv.waitKey(25)\n",
    "            cv.imshow('',image)\n",
    "            action_values = agent.q_value(state, eps)\n",
    "            action = agent.decide(action_values, eps)\n",
    "            \n",
    "            _, reward, done, _ , _ = env.step(action)\n",
    "            if done:\n",
    "                reward = -1\n",
    "            image : np.ndarray = env.render()\n",
    "            recorder.append(preprocess_image(image))\n",
    "            next_state = recorder.numpy()\n",
    "\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \"\"\" === this step has finished === \"\"\"\n",
    "            wandb.log({'action':action, 'reward': reward, 'eps': eps})\n",
    "            wandb.log({f'action_values[{i}]':q for i, q in enumerate(action_values.cpu().numpy().flatten()) })\n",
    "            \"\"\" === next iteration === \"\"\"\n",
    "            state = next_state\n",
    "            accumulate_reward += reward\n",
    "            rounds += 1\n",
    "            if done:\n",
    "                wandb.log({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                print({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                break\n",
    "        if episode % UPDATE_EVERY == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            loss = agent.learn_from_experience()\n",
    "            wandb.log({'loss':loss})\n",
    "            print({'loss':loss})\n",
    "        if episode % TARGET_UPDATE_EVERY == 0:\n",
    "            agent.soft_update()\n",
    "        scores_window.append(accumulate_reward)\n",
    "        scores.append(accumulate_reward)\n",
    "        num_rounds.append(rounds)\n",
    "        eps = max(eps_end, eps*eps_decay)\n",
    "        if episode % 100 == 0:\n",
    "            print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores, num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "0 6.0\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(3.9435e+10, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1737441.2500, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1356339.7500, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(24842.7461, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'loss': tensor(60.8660, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.7943, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(82.4963, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 54, 'accumulate_reward': 52.0}\n",
      "{'loss': tensor(1.5969, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.5132, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 66, 'accumulate_reward': 64.0}\n",
      "{'loss': tensor(1.4848, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.4387, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(2.2378, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(2.3664, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.3713, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'loss': tensor(2.4668, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(2.2049, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(1.8904, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(1.8582, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(1.5367, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.5449, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.4856, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.9457, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(1.4249, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.8476, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(1.4040, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.4484, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'loss': tensor(1.0167, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.8824, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(1.1822, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(0.6992, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.6500, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(0.6071, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.9129, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.7452, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(0.4825, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(0.5772, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.6672, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.5173, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(0.2690, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(1.0368, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.7943, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.3585, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'loss': tensor(0.1639, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 34, 'accumulate_reward': 32.0}\n",
      "{'loss': tensor(1.7510, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.4950, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(1.9788, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 33, 'accumulate_reward': 31.0}\n",
      "{'loss': tensor(2.8841, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.4809, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(1.1203, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.1873, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "100 14.32\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(2.0074, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(2.4360, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(2.4441, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(2.0004, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 37, 'accumulate_reward': 35.0}\n",
      "{'loss': tensor(2.0606, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.8549, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.7590, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.1624, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.8687, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 36, 'accumulate_reward': 34.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.8707, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(1.9750, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.8498, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(4.1018, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(4.2372, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(2.0902, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.3845, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(2.7645, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 4, 'accumulate_reward': 2.0}\n",
      "{'loss': tensor(3.3007, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(4.4664, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(4.3847, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 40, 'accumulate_reward': 38.0}\n",
      "{'loss': tensor(3.7515, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(3.5334, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(3.9667, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(2.9821, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(3.2285, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.8327, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(2.4816, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.9650, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0440, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(4.3522, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(2.3598, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.0061, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(2.7145, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(1.7861, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(3.3713, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.6538, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(3.2531, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.8357, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.7320, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'loss': tensor(1.5667, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.7211, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(2.2575, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.9992, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(1.5869, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(2.1808, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'loss': tensor(1.0245, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(1.5668, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(2.1335, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.8109, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(0.7839, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "200 13.06\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.8609, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 35, 'accumulate_reward': 33.0}\n",
      "{'loss': tensor(1.5908, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.6142, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(4.8536, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(2.4702, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.3540, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(4.6463, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(3.7694, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(3.6470, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.3758, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.4584, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.9711, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'loss': tensor(1.9657, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(3.6736, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(4.1950, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(3.5645, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(4.2953, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(2.4428, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 55, 'accumulate_reward': 53.0}\n",
      "{'loss': tensor(1.8157, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.0274, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(3.2652, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.9251, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.2260, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(4.0945, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.2345, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(4.5912, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(4.4023, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.1335, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'loss': tensor(2.2404, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(3.5653, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(3.2845, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.5743, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(3.4495, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.7110, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(1.6242, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 32, 'accumulate_reward': 30.0}\n",
      "{'loss': tensor(0.4599, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(1.4681, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(1.4066, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.1616, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.8238, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(2.1846, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(2.5442, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(1.2174, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.8711, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(1.4478, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.8628, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(0.8330, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.8213, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(1.1294, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "300 12.75\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(1.0397, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.1941, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(7.7171, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(7.0875, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(5.7002, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(5.4071, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.9299, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(4.8168, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(7.5064, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(3.7286, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(3.3210, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(3.5082, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(4.0576, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 4, 'accumulate_reward': 2.0}\n",
      "{'loss': tensor(4.5725, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(1.7546, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'loss': tensor(1.6070, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(3.9869, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(6.6656, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(7.0200, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 36, 'accumulate_reward': 34.0}\n",
      "{'loss': tensor(6.9979, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(3.8870, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.9351, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.4429, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.2680, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.1938, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.2565, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.3056, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.4820, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.2179, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.1721, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.5814, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.7663, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 40, 'accumulate_reward': 38.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.8881, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.5070, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.9710, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(1.7707, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(2.2524, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(2.5299, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(2.1977, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(2.0632, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(1.5600, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.9597, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 40, 'accumulate_reward': 38.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.3794, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.4098, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.5216, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.3796, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.2150, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 32, 'accumulate_reward': 30.0}\n",
      "{'loss': tensor(1.2700, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(0.9098, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.4559, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "400 12.42\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.0507, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(0.9771, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(1.7851, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.7816, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.4971, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 41, 'accumulate_reward': 39.0}\n",
      "{'loss': tensor(2.1775, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'loss': tensor(0.6110, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.0928, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.4667, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(0.6401, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(1.9693, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 35, 'accumulate_reward': 33.0}\n",
      "{'loss': tensor(1.5062, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.9742, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.9890, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.9344, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'loss': tensor(1.7686, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.3194, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.8194, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.1758, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.5096, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(0.8859, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.1981, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(1.9308, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.2311, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(0.1941, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.2467, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 35, 'accumulate_reward': 33.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.8856, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(21.3845, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1186, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.8548, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(0.8665, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.7596, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 32, 'accumulate_reward': 30.0}\n",
      "{'loss': tensor(1.3739, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.3973, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(0.9480, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.4387, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.9407, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(2.4062, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.4243, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.9408, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(0.4674, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 37, 'accumulate_reward': 35.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(18.5838, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(18.0888, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(16.6759, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(14.7256, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 49, 'accumulate_reward': 47.0}\n",
      "{'loss': tensor(12.5566, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(10.0767, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(6.0504, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(4.6579, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "500 13.07\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(5.7852, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(4.3279, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(9.4745, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(16.4821, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(15.1955, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(3.5904, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(5.1513, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(4.5805, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(22.8399, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(18.4281, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(14.2489, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(14.9609, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'loss': tensor(7.9452, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(5.0277, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(3.7347, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(3.0915, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'loss': tensor(2.0972, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.7905, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 40, 'accumulate_reward': 38.0}\n",
      "{'loss': tensor(0.3406, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(0.6965, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.8334, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.9173, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.6184, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.6631, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(1.4961, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.1875, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(1.5042, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.4147, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(1.2846, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.7672, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.2731, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 23, 'accumulate_reward': 21.0}\n",
      "{'loss': tensor(1.7263, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.4098, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.5939, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.6996, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(1.7678, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.6871, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 37, 'accumulate_reward': 35.0}\n",
      "{'loss': tensor(1.5718, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.3701, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.2011, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.2616, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.0619, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.4167, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 39, 'accumulate_reward': 37.0}\n",
      "{'loss': tensor(1.0000, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 32, 'accumulate_reward': 30.0}\n",
      "{'loss': tensor(1.1734, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.1065, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.9813, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.7677, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 37, 'accumulate_reward': 35.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(1.2355, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.8442, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "600 12.5\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.5279, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.8639, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 30, 'accumulate_reward': 28.0}\n",
      "{'loss': tensor(0.6776, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(0.7318, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 25, 'accumulate_reward': 23.0}\n",
      "{'loss': tensor(0.4180, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(1.1222, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 38, 'accumulate_reward': 36.0}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'loss': tensor(0.3764, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.6290, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.3316, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(0.6081, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.8506, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.8609, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.5980, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.1369, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.2978, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.1651, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(0.5669, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.8649, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(1.1495, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(0.8482, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(17.7412, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(17.7596, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(17.1508, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(16.2769, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(13.7996, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(12.6319, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(11.5756, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(9.3657, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(7.7783, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(7.2535, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(5.8381, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(5.4417, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(3.0095, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(2.9013, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 47, 'accumulate_reward': 45.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.8696, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(2.0947, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(3.9313, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'loss': tensor(5.4113, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(3.8271, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(6.4515, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(4.7010, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(1.7093, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(7.8531, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(3.7499, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(2.9096, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(2.9561, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 27, 'accumulate_reward': 25.0}\n",
      "{'loss': tensor(4.5024, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.2579, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(3.1852, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(1.9942, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "700 12.55\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(1.8620, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(12.3060, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(11.4964, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(11.9019, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 4, 'accumulate_reward': 2.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(8.5126, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 35, 'accumulate_reward': 33.0}\n",
      "{'loss': tensor(7.8726, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(6.5888, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(5.5159, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(4.4432, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(3.3405, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(2.9905, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(2.4683, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.9359, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(1.8861, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(1.7771, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(1.5242, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.9965, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'rounds': 40, 'accumulate_reward': 38.0}\n",
      "{'loss': tensor(0.9365, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.9062, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 18, 'accumulate_reward': 16.0}\n",
      "{'loss': tensor(0.6574, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.6724, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.5769, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.5979, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(4.0346, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 16, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(4.2335, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(4.2866, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(3.5021, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(3.4637, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(3.4449, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(3.5041, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(2.7059, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(2.6459, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 20, 'accumulate_reward': 18.0}\n",
      "{'loss': tensor(2.5805, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(2.7588, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(1.7534, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'loss': tensor(1.8309, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 43, 'accumulate_reward': 41.0}\n",
      "{'loss': tensor(1.8131, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.6478, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 31, 'accumulate_reward': 29.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(1.0004, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 32, 'accumulate_reward': 30.0}\n",
      "{'loss': tensor(0.8733, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 39, 'accumulate_reward': 37.0}\n",
      "{'loss': tensor(0.9316, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 21, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(1.0294, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.2816, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.4441, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.5769, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.8276, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.6797, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(1.2797, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 15.0}\n",
      "{'rounds': 26, 'accumulate_reward': 24.0}\n",
      "{'loss': tensor(0.8245, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.7577, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "800 12.91\n",
      "{'rounds': 29, 'accumulate_reward': 27.0}\n",
      "{'rounds': 7, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.6088, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'rounds': 28, 'accumulate_reward': 26.0}\n",
      "{'loss': tensor(0.4058, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 4.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.3116, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.4834, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'rounds': 48, 'accumulate_reward': 46.0}\n",
      "{'loss': tensor(0.7784, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 24, 'accumulate_reward': 22.0}\n",
      "{'rounds': 10, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.7045, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.5197, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 22, 'accumulate_reward': 20.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.6717, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(1.1067, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.9781, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 10.0}\n",
      "{'rounds': 37, 'accumulate_reward': 35.0}\n",
      "{'loss': tensor(0.6380, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 19, 'accumulate_reward': 17.0}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n",
      "{'loss': tensor(0.8889, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(1.3314, device='cuda:0', grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scores, num_rounds \u001b[39m=\u001b[39m train(agent, n_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, max_time_step\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m, eps_start\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, eps_end\u001b[39m=\u001b[39;49m\u001b[39m0.6\u001b[39;49m, eps_decay\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[23], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     20\u001b[0m rounds \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m time_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_time_step):\n\u001b[1;32m---> 22\u001b[0m     cv\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m25\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m     cv\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,image)\n\u001b[0;32m     24\u001b[0m     action_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mq_value(state, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, num_rounds = train(agent, n_episodes=1000, max_time_step=3000, eps_start=1.0, eps_end=0.6, eps_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image : np.ndarray = env.render(mode='rgb_array')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d155ddd594e93c13dc604d9fde6a83ce6733a6c3fc0471778cef1ca6f99b89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
