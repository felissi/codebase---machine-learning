{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "from collections import deque\n",
    "from typing import Optional, Iterable\n",
    "import random\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(deque):\n",
    "    def __init__(self, iterable: Optional[Iterable]=None, maxlen: Optional[int]=None):\n",
    "        if iterable is None:\n",
    "            super(Recorder, self).__init__(maxlen=maxlen)\n",
    "        else:\n",
    "            super(Recorder, self).__init__(iterable, maxlen=maxlen)\n",
    "\n",
    "    def tensor(self):\n",
    "        \"\"\" Give [B, C, H, W] tensor for images \n",
    "        e.g. [H, W]x4 -> [4, H, W]\n",
    "        \"\"\"\n",
    "        return torch.stack(tuple(torch.tensor(x, device=device) for x in self))\n",
    "    \n",
    "    def numpy(self)->np.ndarray:\n",
    "        return self.tensor().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIDTH  = 240\n",
    "HEIGHT = 160\n",
    "def preprocess_image(image: np.ndarray, width=WIDTH, height=HEIGHT):\n",
    "    _, gray = cv.threshold(cv.cvtColor(image, cv.COLOR_BGR2GRAY), 200, 255,cv.THRESH_BINARY)\n",
    "    # gray:np.ndarray = gray[(gray.shape[0]-WIDTH)//2:(gray.shape[0]-WIDTH)//2+WIDTH,(gray.shape[1]-HEIGHT)//2:(gray.shape[1]-HEIGHT)//2+HEIGHT]\n",
    "    gray = cv.resize(gray, (width, height))\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "recorder = Recorder(maxlen=4)\n",
    "for _ in range(4):\n",
    "    env.step(random.choice([0,1]))\n",
    "    image : np.ndarray = env.render(mode='rgb_array')\n",
    "    recorder.append(preprocess_image(image))\n",
    "i:np.ndarray = env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 240)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_image(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "replay = ReplayBuffer(buffer_size=100, batch_size=32)\n",
    "for _ in range(100):\n",
    "    replay.add(recorder.numpy(), 0, 0, recorder.numpy(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 160, 240])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recorder.tensor().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 160, 240])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay.sample().state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407],\n",
       "        [ 0.1227, -0.3407]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Model import QNetwork\n",
    "model = QNetwork(action_size=2)\n",
    "model(replay.sample().state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE         = 100\n",
    "BATCH_SIZE          = 32\n",
    "GAMMA               = 0.99  # discount factor\n",
    "TAU                 = 1e-3  # soft update of target parameter\n",
    "LEARNING_RATE       = 1e-2\n",
    "UPDATE_EVERY        = 10    # how often to update the local\n",
    "TARGET_UPDATE_EVERY = 50    # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import Agent\n",
    "agent = Agent('not_used', 2, LEARNING_RATE, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(5, 5), stride=(3, 3))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=52992, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(replay.memory[0].state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075],\n",
       "        [0.1505, 0.4075]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local(replay.sample().state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0227, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(replay.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.step(*replay.memory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-183329.9062,   89266.1094]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_value(replay.memory[0].state, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    num_rounds = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    env = gym.make('CartPole-v1')\n",
    "    recorder = Recorder(maxlen=4)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        env.reset()\n",
    "        # Get first 3 frames\n",
    "        for _ in range(4):\n",
    "            env.step(random.choice([0,1]))\n",
    "            image : np.ndarray = env.render(mode='rgb_array')\n",
    "            recorder.append(preprocess_image(image))\n",
    "        state = recorder.numpy()\n",
    "        accumulate_reward = 0\n",
    "        rounds = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            action_values = agent.q_value(state, eps)\n",
    "            action = agent.decide(action_values, eps)\n",
    "            \n",
    "            _, reward, done, _ = env.step(action)\n",
    "            image : np.ndarray = env.render(mode='rgb_array')\n",
    "            recorder.append(preprocess_image(image))\n",
    "            next_state = recorder.numpy()\n",
    "\n",
    "            cv.waitKey(25)\n",
    "            cv.imshow('',image)\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \"\"\" === this step has finished === \"\"\"\n",
    "            # wandb.log({'action':action, 'reward': reward, 'eps': eps})\n",
    "            # wandb.log({f'action_values[{i}]':q for i, q in enumerate(action_values.cpu().numpy().flatten()) })\n",
    "            \"\"\" === next iteration === \"\"\"\n",
    "            state = next_state\n",
    "            accumulate_reward += reward\n",
    "            rounds += 1\n",
    "            if done:\n",
    "                # wandb.log({'rounds':rounds,'accumulate_reward':accumulate_reward, 'max_number':np.max(state)})\n",
    "                print({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                break\n",
    "        if episode % UPDATE_EVERY == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            loss = agent.learn_from_experience()\n",
    "            # wandb.log({'loss':loss})\n",
    "            print({'loss':loss})\n",
    "        if episode % TARGET_UPDATE_EVERY == 0:\n",
    "            agent.soft_update()\n",
    "        scores_window.append(accumulate_reward)\n",
    "        scores.append(accumulate_reward)\n",
    "        num_rounds.append(rounds)\n",
    "        eps = max(eps_end, eps*eps_decay)\n",
    "        if episode % 100 == 0:\n",
    "            print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores, num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0714, grad_fn=<MseLossBackward0>)}\n",
      "0 8.0\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 44, 'accumulate_reward': 44.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 33, 'accumulate_reward': 33.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 53, 'accumulate_reward': 53.0}\n",
      "{'rounds': 27, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(0.0401, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 21, 'accumulate_reward': 21.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 25, 'accumulate_reward': 25.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 31, 'accumulate_reward': 31.0}\n",
      "{'loss': tensor(0.0386, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 27, 'accumulate_reward': 27.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 22, 'accumulate_reward': 22.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 25, 'accumulate_reward': 25.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 22, 'accumulate_reward': 22.0}\n",
      "{'loss': tensor(0.0542, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 28, 'accumulate_reward': 28.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 23, 'accumulate_reward': 23.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 29, 'accumulate_reward': 29.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 26, 'accumulate_reward': 26.0}\n",
      "{'loss': tensor(0.0238, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 45, 'accumulate_reward': 45.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.0697, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 26, 'accumulate_reward': 26.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 27, 'accumulate_reward': 27.0}\n",
      "{'loss': tensor(0.0512, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 22, 'accumulate_reward': 22.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0364, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 23, 'accumulate_reward': 23.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0397, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0572, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 28, 'accumulate_reward': 28.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0746, grad_fn=<MseLossBackward0>)}\n",
      "100 15.22\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.0920, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 25, 'accumulate_reward': 25.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0536, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0373, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(7.0900, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 26, 'accumulate_reward': 26.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0621, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0906, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.1256, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 21, 'accumulate_reward': 21.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.1171, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 24, 'accumulate_reward': 24.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.1394, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.1220, grad_fn=<MseLossBackward0>)}\n",
      "200 10.93\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0986, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(0.1206, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.1108, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0876, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.0457, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'loss': tensor(0.0627, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0879, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0550, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0680, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1443, grad_fn=<MseLossBackward0>)}\n",
      "300 10.11\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.0262, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.2196, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0554, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.0959, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.1423, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0376, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1194, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0684, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1454, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0522, grad_fn=<MseLossBackward0>)}\n",
      "400 10.23\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 4, 'accumulate_reward': 4.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0666, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0799, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.1272, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 4, 'accumulate_reward': 4.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0993, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0806, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0987, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.0870, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.0746, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0614, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1240, grad_fn=<MseLossBackward0>)}\n",
      "500 9.52\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0915, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'loss': tensor(0.0369, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1067, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0753, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'loss': tensor(0.0590, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0826, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 19, 'accumulate_reward': 19.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1084, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1439, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.1134, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.1139, grad_fn=<MseLossBackward0>)}\n",
      "600 10.46\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.1004, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1411, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0789, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1113, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.0689, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.0843, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'loss': tensor(0.0745, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1454, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.0611, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 4, 'accumulate_reward': 4.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.0525, grad_fn=<MseLossBackward0>)}\n",
      "700 10.31\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.0650, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.0787, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.1254, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.0305, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1528, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1151, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.0826, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.0873, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 4, 'accumulate_reward': 4.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.1086, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1347, grad_fn=<MseLossBackward0>)}\n",
      "800 9.98\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'loss': tensor(0.1910, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1175, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 4, 'accumulate_reward': 4.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.0882, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.1394, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1140, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.2187, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'loss': tensor(0.0479, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'loss': tensor(0.1456, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'loss': tensor(0.1681, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 20, 'accumulate_reward': 20.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.1189, grad_fn=<MseLossBackward0>)}\n",
      "900 9.88\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.1524, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 5, 'accumulate_reward': 5.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'loss': tensor(0.1257, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.1727, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'loss': tensor(0.0871, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'loss': tensor(0.0693, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.0728, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 13, 'accumulate_reward': 13.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'loss': tensor(0.1467, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 18, 'accumulate_reward': 18.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 10, 'accumulate_reward': 10.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'loss': tensor(0.1397, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 7, 'accumulate_reward': 7.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 12, 'accumulate_reward': 12.0}\n",
      "{'loss': tensor(0.1468, grad_fn=<MseLossBackward0>)}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 17, 'accumulate_reward': 17.0}\n",
      "{'rounds': 6, 'accumulate_reward': 6.0}\n",
      "{'rounds': 8, 'accumulate_reward': 8.0}\n",
      "{'rounds': 14, 'accumulate_reward': 14.0}\n",
      "{'rounds': 11, 'accumulate_reward': 11.0}\n",
      "{'rounds': 15, 'accumulate_reward': 15.0}\n",
      "{'rounds': 9, 'accumulate_reward': 9.0}\n",
      "{'rounds': 16, 'accumulate_reward': 16.0}\n"
     ]
    }
   ],
   "source": [
    "scores, num_rounds = train(agent, n_episodes=1000, max_time_step=3000, eps_start=1.0, eps_end=0.01, eps_decay=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "image : np.ndarray = env.render(mode='rgb_array')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f972118ac7c6a56642233e9551f2790bbdf3f6ed0ba1febcedad4f4ce41f7f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
