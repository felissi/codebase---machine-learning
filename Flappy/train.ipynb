{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import imageio\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from collections import deque\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"game/\")\n",
    "from game import wrapped_flappy_bird as game\n",
    "\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), \"../utils\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH  = 80\n",
    "IMG_HEIGHT = 80\n",
    "IMG_DEPTH  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE         = 100\n",
    "BATCH_SIZE          = 64\n",
    "GAMMA               = 0.90  # discount factor\n",
    "TAU                 = 1e-3  # soft update of target parameter\n",
    "LEARNING_RATE       = 5e-4\n",
    "UPDATE_EVERY        = 3     # how often to update the local\n",
    "TARGET_UPDATE_EVERY = 9     # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_array(action: int):\n",
    "    if action: return np.array([0,1])\n",
    "    return np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    num_rounds = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for episode in range(n_episodes):\n",
    "        env = game.GameState()\n",
    "        state, reward, done = env.frame_step(np.array([0,1]))\n",
    "        accumulate_reward = 0\n",
    "        rounds = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            action_values = agent.q_value(state, eps)\n",
    "            action = agent.decide(action_values, eps)\n",
    "            next_state, reward, done = env.frame_step(action_to_array(action))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \"\"\" === this step has finished === \"\"\"\n",
    "            # wandb.log({'action':action, 'reward': reward, 'eps': eps})\n",
    "            # wandb.log({f'action_values[{i}]':q for i, q in enumerate(action_values.cpu().numpy().flatten()) })\n",
    "            \"\"\" === next iteration === \"\"\"\n",
    "            state = next_state\n",
    "            accumulate_reward += reward\n",
    "            rounds += 1\n",
    "            if done:\n",
    "                # wandb.log({'rounds':rounds,'accumulate_reward':accumulate_reward, 'max_number':np.max(state)})\n",
    "                print({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                break\n",
    "        scores_window.append(accumulate_reward)\n",
    "        scores.append(accumulate_reward)\n",
    "        num_rounds.append(rounds)\n",
    "        eps = max(eps_end, eps-eps_decay)\n",
    "        if episode % UPDATE_EVERY == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            print('update local')\n",
    "            loss = agent.learn_from_experience()\n",
    "            # wandb.log({'loss':loss})\n",
    "        if episode % TARGET_UPDATE_EVERY == 0:\n",
    "            print('update target')\n",
    "            agent.soft_update()\n",
    "        if episode % 100 == 0:\n",
    "            print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores, num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent('not_used',2, LEARNING_RATE, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=294912, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = flappy()\n",
    "# mod.play(\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = game.GameState()\n",
    "state, reward, done = env.frame_step(np.array([0,1]))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,  84.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,  84.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,  84.],\n",
       "          ...,\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.],\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.],\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.]],\n",
       "\n",
       "         [[135., 135., 135.,  ..., 135., 135.,  56.],\n",
       "          [135., 135., 135.,  ..., 135., 135.,  56.],\n",
       "          [135., 135., 135.,  ..., 135., 135.,  56.],\n",
       "          ...,\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.],\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.],\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.]],\n",
       "\n",
       "         [[147., 147., 147.,  ..., 147., 147.,  71.],\n",
       "          [147., 147., 147.,  ..., 147., 147.,  71.],\n",
       "          [147., 147., 147.,  ..., 147., 147.,  71.],\n",
       "          ...,\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.],\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.],\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch = torch.tensor(state, device=device).float().unsqueeze(0).permute(0, 3, 2, 1)\n",
    "single_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.4424]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(single_batch.shape)\n",
    "agent.qnetwork_local(torch.tensor(state, device=device).float().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "update local\n",
      "update target\n",
      "0 5.499999999999993\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "update local\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "update local\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n",
      "{'rounds': 66, 'accumulate_reward': 5.499999999999993}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\felixwong\\Desktop\\py alg practice\\Qlearning\\Flappy\\train.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(agent, n_episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, max_time_step\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, eps_start\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, eps_end\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m, eps_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\felixwong\\Desktop\\py alg practice\\Qlearning\\Flappy\\train.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rounds \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m time_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_time_step):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     action_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mq_value(state, eps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mdecide(action_values, eps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/felixwong/Desktop/py%20alg%20practice/Qlearning/Flappy/train.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     next_state, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mframe_step(action_to_array(\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\felixwong\\Desktop\\py alg practice\\Qlearning\\Flappy\\Agent.py:85\u001b[0m, in \u001b[0;36mAgent.q_value\u001b[1;34m(self, state, eps, train)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local\u001b[39m.\u001b[39meval()\n\u001b[0;32m     84\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 85\u001b[0m     action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnetwork_local(state)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqnetwork_local\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m action_values\n",
      "File \u001b[1;32mc:\\Users\\felixwong\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\felixwong\\Desktop\\py alg practice\\Qlearning\\Flappy\\Model.py:22\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# flip [B, W, H, C] -> [B, C, H, W]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m---> 22\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mrelu(x)\n\u001b[0;32m     23\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmax_pool2d(x, \u001b[39m2\u001b[39m) \u001b[39m# -> [B, 32, H/2, W/2]\u001b[39;00m\n\u001b[0;32m     24\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[1;32mc:\\Users\\felixwong\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(agent, n_episodes=100, max_time_step=1000, eps_start=1.0, eps_end=0.05, eps_decay=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f972118ac7c6a56642233e9551f2790bbdf3f6ed0ba1febcedad4f4ce41f7f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
