{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "# import imageio\n",
    "# import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from collections import deque\n",
    "from typing import Optional\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"game/\")\n",
    "from game import wrapped_flappy_bird as game\n",
    "\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), \"../utils\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    key = key or input()\n",
    "except NameError:\n",
    "    key = input('type your wandb api key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelissi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Desktop\\py-practice\\machine_learning\\codebase---machine-learning\\Flappy\\wandb\\run-20230222_010311-nyrnlfqt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felissi/flappy_bird_dqn/runs/nyrnlfqt' target=\"_blank\">effortless-vortex-6</a></strong> to <a href='https://wandb.ai/felissi/flappy_bird_dqn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felissi/flappy_bird_dqn' target=\"_blank\">https://wandb.ai/felissi/flappy_bird_dqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felissi/flappy_bird_dqn/runs/nyrnlfqt' target=\"_blank\">https://wandb.ai/felissi/flappy_bird_dqn/runs/nyrnlfqt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/felissi/flappy_bird_dqn/runs/nyrnlfqt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1bc244c7250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"flappy_bird_dqn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "IMG_WIDTH  = 80\n",
    "IMG_HEIGHT = 80\n",
    "IMG_DEPTH  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE         = 200\n",
    "BATCH_SIZE          = 100\n",
    "GAMMA               = 0.98  # discount factor\n",
    "TAU                 = 1e-3  # soft update of target parameter\n",
    "LEARNING_RATE       = 0.01\n",
    "UPDATE_EVERY        = 2     # how often to update the local\n",
    "TARGET_UPDATE_EVERY = 20     # how often to update the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent('not_used',2, LEARNING_RATE, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(agent.qnetwork_local,log_freq=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_array(action: int):\n",
    "    if action: return np.array([0,1])\n",
    "    return np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: Agent, n_episodes, max_time_step, eps_start, eps_end, eps_decay):\n",
    "    scores = []\n",
    "    num_rounds = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for episode in range(n_episodes):\n",
    "        env = game.GameState()\n",
    "        state, reward, done = env.frame_step(np.array([0,1]))\n",
    "        accumulate_reward = 0\n",
    "        rounds = 0\n",
    "        for time_step in range(max_time_step):\n",
    "            action_values = agent.q_value(state, eps)\n",
    "            action = agent.decide(action_values, eps)\n",
    "            next_state, reward, done = env.frame_step(action_to_array(action))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \"\"\" === this step has finished === \"\"\"\n",
    "            wandb.log({'action':action, 'reward': reward, 'eps': eps})\n",
    "            wandb.log({f'action_values[{i}]':q for i, q in enumerate(action_values.cpu().numpy().flatten()) })\n",
    "            \"\"\" === next iteration === \"\"\"\n",
    "            state = next_state\n",
    "            accumulate_reward += reward\n",
    "            rounds += 1\n",
    "            if done:\n",
    "                wandb.log({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                # print({'rounds':rounds,'accumulate_reward':accumulate_reward})\n",
    "                break\n",
    "        scores_window.append(accumulate_reward)\n",
    "        scores.append(accumulate_reward)\n",
    "        num_rounds.append(rounds)\n",
    "        eps = max(eps_end, eps-eps_decay)\n",
    "        if episode % UPDATE_EVERY == 0 and len(agent.memory) > BATCH_SIZE:\n",
    "            # print('update local')\n",
    "            loss = agent.learn_from_experience()\n",
    "            wandb.log({'loss':loss})\n",
    "        if episode % TARGET_UPDATE_EVERY == 0:\n",
    "            # print('update target')\n",
    "            agent.soft_update()\n",
    "        if episode % 100 == 0:\n",
    "            # print(episode, np.mean(scores_window))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pt')\n",
    "    return scores, num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=294912, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qnetwork_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = flappy()\n",
    "# mod.play(\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        [  0, 135, 147],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        [ 84,  56,  71],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[ 85, 128,  34],\n",
       "        [ 85, 128,  34],\n",
       "        [ 85, 128,  34],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env = game.GameState()\n",
    "state, reward, done = env.frame_step(np.array([0,1]))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 512, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0.,   0.,   0.,  ...,  84.,  84.,  85.],\n",
       "          [  0.,   0.,   0.,  ...,  84.,  84.,  85.],\n",
       "          [  0.,   0.,   0.,  ...,  84.,  84.,  85.],\n",
       "          ...,\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.],\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.],\n",
       "          [222., 222., 222.,  ..., 222., 222., 222.]],\n",
       "\n",
       "         [[135., 135., 135.,  ...,  56.,  56., 128.],\n",
       "          [135., 135., 135.,  ...,  56.,  56., 128.],\n",
       "          [135., 135., 135.,  ...,  56.,  56., 128.],\n",
       "          ...,\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.],\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.],\n",
       "          [216., 216., 216.,  ..., 216., 216., 216.]],\n",
       "\n",
       "         [[147., 147., 147.,  ...,  71.,  71.,  34.],\n",
       "          [147., 147., 147.,  ...,  71.,  71.,  34.],\n",
       "          [147., 147., 147.,  ...,  71.,  71.,  34.],\n",
       "          ...,\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.],\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.],\n",
       "          [149., 149., 149.,  ..., 149., 149., 149.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch = torch.tensor(state, device=device).float().unsqueeze(0).permute(0, 3, 2, 1)\n",
    "single_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.2414, 0.1160]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(single_batch.shape)\n",
    "agent.qnetwork_local(torch.tensor(state, device=device).float().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.800000000000002,\n",
       "  3.0000000000000018,\n",
       "  2.900000000000002,\n",
       "  2.800000000000002,\n",
       "  5.299999999999997,\n",
       "  2.800000000000002,\n",
       "  6.5999999999999925,\n",
       "  6.5999999999999925,\n",
       "  2.900000000000002,\n",
       "  3.5,\n",
       "  3.6999999999999993,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  6.5999999999999925,\n",
       "  2.800000000000002,\n",
       "  3.200000000000001,\n",
       "  5.299999999999997,\n",
       "  6.5999999999999925,\n",
       "  2.900000000000002,\n",
       "  2.800000000000002,\n",
       "  3.1000000000000014,\n",
       "  2.800000000000002,\n",
       "  6.8999999999999915,\n",
       "  2.800000000000002,\n",
       "  4.899999999999999,\n",
       "  2.800000000000002,\n",
       "  2.900000000000002,\n",
       "  2.800000000000002,\n",
       "  2.900000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  1.1000000000000005,\n",
       "  2.800000000000002,\n",
       "  2.700000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.4000000000000017,\n",
       "  2.800000000000002,\n",
       "  2.800000000000002,\n",
       "  2.900000000000002,\n",
       "  2.800000000000002,\n",
       "  2.1000000000000014,\n",
       "  0.7000000000000004,\n",
       "  2.800000000000002,\n",
       "  2.5000000000000018,\n",
       "  2.800000000000002,\n",
       "  0.6000000000000003,\n",
       "  1.0000000000000004,\n",
       "  1.2000000000000006,\n",
       "  2.800000000000002,\n",
       "  1.5000000000000009,\n",
       "  1.9000000000000012,\n",
       "  1.0000000000000004,\n",
       "  2.700000000000002,\n",
       "  2.800000000000002,\n",
       "  0.40000000000000013,\n",
       "  1.8000000000000012,\n",
       "  1.2000000000000006,\n",
       "  2.2000000000000015,\n",
       "  0.8000000000000005,\n",
       "  2.800000000000002,\n",
       "  0.8000000000000005,\n",
       "  2.1000000000000014,\n",
       "  0.40000000000000013,\n",
       "  1.600000000000001,\n",
       "  0.40000000000000013,\n",
       "  1.600000000000001,\n",
       "  1.8000000000000012,\n",
       "  0.8000000000000005,\n",
       "  1.5000000000000009,\n",
       "  1.2000000000000006,\n",
       "  0.40000000000000013,\n",
       "  0.8000000000000005,\n",
       "  0.40000000000000013,\n",
       "  1.2000000000000006,\n",
       "  0.8000000000000005,\n",
       "  0.40000000000000013,\n",
       "  1.2000000000000006,\n",
       "  0.8000000000000005,\n",
       "  0.40000000000000013,\n",
       "  0.8000000000000005,\n",
       "  0.8000000000000005,\n",
       "  0.40000000000000013,\n",
       "  0.40000000000000013,\n",
       "  0.40000000000000013,\n",
       "  0.40000000000000013],\n",
       " [39,\n",
       "  41,\n",
       "  40,\n",
       "  39,\n",
       "  55,\n",
       "  39,\n",
       "  68,\n",
       "  68,\n",
       "  40,\n",
       "  46,\n",
       "  48,\n",
       "  39,\n",
       "  39,\n",
       "  68,\n",
       "  39,\n",
       "  43,\n",
       "  55,\n",
       "  68,\n",
       "  40,\n",
       "  39,\n",
       "  42,\n",
       "  39,\n",
       "  71,\n",
       "  39,\n",
       "  51,\n",
       "  39,\n",
       "  40,\n",
       "  39,\n",
       "  40,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  22,\n",
       "  39,\n",
       "  38,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  39,\n",
       "  35,\n",
       "  39,\n",
       "  39,\n",
       "  40,\n",
       "  39,\n",
       "  32,\n",
       "  18,\n",
       "  39,\n",
       "  36,\n",
       "  39,\n",
       "  17,\n",
       "  21,\n",
       "  23,\n",
       "  39,\n",
       "  26,\n",
       "  30,\n",
       "  21,\n",
       "  38,\n",
       "  39,\n",
       "  15,\n",
       "  29,\n",
       "  23,\n",
       "  33,\n",
       "  19,\n",
       "  39,\n",
       "  19,\n",
       "  32,\n",
       "  15,\n",
       "  27,\n",
       "  15,\n",
       "  27,\n",
       "  29,\n",
       "  19,\n",
       "  26,\n",
       "  23,\n",
       "  15,\n",
       "  19,\n",
       "  15,\n",
       "  23,\n",
       "  19,\n",
       "  15,\n",
       "  23,\n",
       "  19,\n",
       "  15,\n",
       "  19,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(agent, n_episodes=100, max_time_step=1000, eps_start=1.0, eps_end=0.05, eps_decay=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d155ddd594e93c13dc604d9fde6a83ce6733a6c3fc0471778cef1ca6f99b89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
